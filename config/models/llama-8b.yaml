model:
  name: llama-8b
  model_id: meta-llama/Llama-3.1-8B-Instruct
  num_layers: 32
  hidden_dim: 4096
  param_count_b: 8.0
  architecture: llama
  quantization: none
  max_new_tokens: 256
  # Layers to sweep: evenly spaced with emphasis on 2/3 depth (~layer 21)
  default_layers: [6, 9, 12, 15, 18, 21, 24, 28]
